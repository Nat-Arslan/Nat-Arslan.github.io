[ { "title": "Using Obsidian, PostgreSQL and Python to Analyse &amp; Visualise Notes", "url": "/posts/Obsidian-PostgreSQL-Python/", "categories": "", "tags": "python, sql, notetaking", "date": "2022-10-30 00:00:00 +0200", "snippet": "Github repository for this project.I use Obsidian to write down notes and my journal in markdown format. The app allows me to create relational notes. I can link these notes to each other, graph this relationship and use tags or dosens of plugins to make sense of my notes. But I always felt I needed to see more patterns and connections. And what is a better tool than Python to achieve this?There is an Obsidian plugin which makes it easy to send the metadata (YAML) of each file to PostgreSQL database. From there on I can connect to the database and analyse my journal entries with Python. Here is my brief workflow: Keep taking notes in Obsidian Important to have consistent metadata (YAML) section. Some example things that can be tracked/recorded are: date, mood, weather, location etc. Download the PostgreSQL plugin for Obsidian After taking a note (journal, meeting etc) use this plugin to send the files metadata to PostgreSQL database. Use SQL in Python to analyse and visualise notes. In this Github repo you can follow all the steps.The steps in Python are briefly: Connect to the PostgreSQL database in jupyter notebook (python): ¬† %sql postgresql://username:password@host:port/database Create an SQL query. Example: query = ‚Äò‚Äô‚Äô SELECT dataview_data -&amp;gt; ‚Äòdate‚Äô as date, dataview_data -&amp;gt; ‚Äòtags‚Äô as tags, dataview_data -&amp;gt; ‚Äòmood‚Äô as mood, dataview_data -&amp;gt; ‚Äògeom‚Äô as geom FROM obsidian.file; ‚Äò‚Äô‚Äô Convert the data from the database into pandas using an SQL query and SQLAlchemy engine: df = pd.read_sql(query, engine) Edit dataframe if necessary Convert dataframe to GeoPandas and map with IpyLeaflet Plot data (Optional) Use Streamlit to create a dashboard if you preferTo reveal further connection between the notes, one can even use dataframe.corr() function to see the correlation between metadata fields. This can help answer questions like ‚ÄúDo I feel better on the days where I have a walk?‚Äù.I know there is a lot of benefit of keeping a hand written journal. But I need to be able to analyse the information that I‚Äôm documenting. Sending data to PostgreSQL and then to Python proved to be a very efficient way to do this." }, { "title": "Interactive Webmap with Django, Leaflet and PostGIS", "url": "/posts/Interactive-Webmap-Django/", "categories": "Coding, Interactive Maps", "tags": "python, webdev, leaflet, django, postgis, earthquake, javascript", "date": "2022-08-21 00:00:00 +0200", "snippet": "Being trained as a GIS analyst, I can see the advantages of acquiring web development skills. So, last week I decided to create my very first interactive web map with Django, Leaflet and PostGIS. In the clickable map you can display information in the popups and the marker size is dependent on the earthquake magnitude.In this jupyter notebook you can see the steps I followed and all the notes I took in order to understand the process of : Downloading and activating PostgreSQL and PostGIS Creating a Django web app Uploading an excel file to the server Displaying this data on a map Adding popup on the markers Styling the map marker Change icon style to circle Display the circle size dependent on earthquake magnitude This was tremendously fun and I can see that I have a long way to go. I would like to repeat a similar project to understand all the steps and how all the components are connected to each other.If you are curious here is the github page for this project" }, { "title": "Visualising Bathymetry Data With Python", "url": "/posts/GEBCO/", "categories": "Bathymetry", "tags": "geography, python, satellite, bathymetry", "date": "2022-05-01 00:00:00 +0200", "snippet": "In 2020, I wrote my thesis where I created a turbidity risk index for seagrass beds in Tsimipaika Bay in northwestern Madagascar. In the beginning of my research I used GEBCO data to observe the bathymetry of the area. Below is the code I used.What‚Äôs next? Plotting a 3D surface model from the data. After converting the netCDF data to dataframe, I couldn‚Äôt create a 3D plot. So this can be the next challenge.Sources Github Link1 Github Link2Data Download GEBCOImport and Check the Dataimport xarray as xrimport osimport pandas as pd# reads a single netCDF fileda = xr.open_dataset(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Input/GEBCO_Northwest Madagascar/gebco_2021_n-11.800086853778835_s-14.190019295022976_w45.94407106576158_e49.25297672806889.nc&#39;)# print dimension names of the data arrayda.dimsFrozen({&#39;lat&#39;: 574, &#39;lon&#39;: 794})# variables in the datasetda.data_varsData variables: elevation (lat, lon) int16 ...# create a list of the coordinates of a given dimension of the data array as a python list# limiting to the first 3 items# Some notes# &quot;xarraydataarry.values &amp;gt; &quot;.values&quot; converts to a numpy array# &quot;.tolist()&quot; is a method to convert the numpy array to a python list# list[0:3] or written as &quot;list[:3]&quot; displays list item number &quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;.da.coords[&#39;lat&#39;].values.tolist()[0:4][-14.189583333333331, -14.185416666666669, -14.181250000000006, -14.177083333333329]Subset and Plot# select the variable and plotda.elevation.plot()plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja1.jpg&#39;)# plot a proflie across the grid at a given latitudeda.elevation.sel(lat=-12.4,method=&#39;nearest&#39;).plot();plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja2.jpg&#39;)# Subset: selecting a regionda.elevation.sel(lon=slice(47,49),lat=slice(-14,-12)).plot(cmap=&#39;Spectral_r&#39;)plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja3.jpg&#39;)# export the DEM profile to a csv making sure the columns are order by lon,lat,depth ie x,y,z# create a pandas dataframe for a given index value the latitude nearest -13 degrees)df = da.elevation.sel(lat=-13,method=&#39;nearest&#39;).to_dataframe(name=&#39;z&#39;)# we don&#39;t need an index for x,y,z datadf = df.reset_index()# renmae the column headers in the pandas dataframedf.columns = [&#39;x&#39;,&#39;y&#39;,&#39;z&#39;]# print the top 5 rows#df.head(5)import matplotlib.pyplot as plt# get funky with key word arguements (kwargs), e.g. with specifying lists of arguements# note the number of items needs to stay the same between arguements.# there&#39;s plenty of functionality, e.g. list colours accepting different types of input# So here, three elevation contours are specifed, with different colours applied, and different line weights#da.elevation.plot.contour(colors=[&#39;blue&#39;,&#39;#000000&#39;,&#39;r&#39;],levels=[-1000,0,500],linewidths=[0.2,1,0.2]);plt.figure(figsize=(20,20))da.elevation.plot.contour(colors=[&#39;#e0efc0&#39;, &#39;#e0efc0&#39;, &#39;#f7be9e&#39;,&#39;#f88989&#39;, &#39;#a7a5a5&#39;, &#39;#b5ece0&#39;, &#39;#b5ece0&#39;], levels=[-1000, -750, -500, 0, 500, 750, 1000], linewidths=[2, 1, 1, 2, 1, 1, 2]);plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja4.jpg&#39;)# plot a histogram of data values in the data array.# Notes# * Use a semi-colon to prevent printing the array data out# * Instantiating the plot method will initalise the figure automatically#da.elevation.plot.hist();#da.elevation.sel(lat=-13,method=&#39;nearest&#39;).plot();# create a figure objectf = plt.figure(figsize=(15, 15))# plot 1# plot with the xarray defaultsax1 = f.add_subplot(2, 2, 1) # this adds a &quot;subplot&quot; (no. of rows, no of cols, index number)da.elevation.plot(cmap=&#39;viridis&#39;);# plot 2: plot with xarray default but specify a number of levels. ax2 = f.add_subplot(2, 2, 2)da.elevation.plot(cmap=&#39;viridis&#39;,levels=7);plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja5.jpg&#39;)# create a figure objectf = plt.figure(figsize=(15, 15))# plot 1# plot with the xarray defaultsax1 = f.add_subplot(2, 2, 1) # this adds a &quot;subplot&quot; (no. of rows, no of cols, index number)da.elevation.plot.contourf()# plot 2# call the matplotlib pcolormesh plot type# notice there are minimal defaults, no axis labels, no colorbar.ax2 = f.add_subplot(2, 2, 2)da.elevation.plot.contourf(cmap=&#39;viridis&#39;)# plot 3# plot with xarray filled contour plotax3 = f.add_subplot(2, 2, 3)da.elevation.plot.contourf(cmap=&#39;terrain&#39;,levels=12)# plot 4# plot with xarray default but specify a number of levels. ax4 = f.add_subplot(2, 2, 4)da.elevation.plot.contourf(colors=[&#39;#333399&#39;,&#39;#4575b4&#39;,&#39;#74add1&#39;,&#39;#abd9e9&#39;,&#39;#e0f3f8&#39;,&#39;#c5f38d&#39;,&#39;#e2da89&#39;,&#39;#aa926b&#39;,&#39;#ffffff&#39;], levels=[-4000,-2000,-1000,-500,-100,0,500,1000,2000], extend=&#39;neither&#39; )plt.savefig(&#39;/Users/nat/Desktop/Geo_Projects/GEBCO/Output/Gebco_Ambanja6.jpg&#39;)Linked Sources Thesis github page" }, { "title": "Gender Gap in Nature History Books (Method: Preferred)", "url": "/posts/Gender-Gap-In-Nature-History-Books-NLTK/", "categories": "Natural Science, Gender", "tags": "gender, books, nature-history, python", "date": "2022-02-28 00:00:00 +0100", "snippet": "1 About the projectCOMING SOON ü§ì2 Code# Source: https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/# importing librariesimport randomfrom nltk.corpus import namesimport nltkimport pandas as pdBook DepositoryThis is where I have the nature history writers data here is the code for getting this data.# Import Bookdepository CSV#authors = pd.read_csv(&quot;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Bookdepository/NaturalHistory-Bookdepository-2021.csv&quot;, dtype=str)authors = pd.read_csv(&quot;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Bookdepository/NaturalHistory-Bookdepository-All.csv&quot;, dtype=str)authors.head(5) Unnamed: 0 authors titles date year 0 0 Stephen Hawking A Brief History Of Time 20 Jan 2015 2015 1 1 James Bowen A Street Cat Named Bob 23 Jan 2013 2013 2 2 Peter Wohlleben The Hidden Life of Trees 22 Nov 2019 2019 3 3 Raynor Winn The Salt Path 31 Jan 2019 2019 4 4 Catherine D. Hughes Little Kids First Big Book of Dinosaurs 16 Aug 2018 2018 # Create a colum for first namesauthors[&quot;FirstName&quot;] = &quot;&quot;authors[&#39;FirstName&#39;] = authors[&#39;authors&#39;].str.split(&quot; &quot;)#1, expand=True#authors[&#39;FirstName&#39;][1][0] # &#39;Jeremy&#39;# Create an empty column for genderauthors[&quot;Gender&quot;] = &quot;&quot;# Drop unnecessary columnsauthors.drop(&#39;Unnamed: 0&#39;, axis=1, inplace=True)authors.head(5) authors titles date year FirstName Gender 0 Stephen Hawking A Brief History Of Time 20 Jan 2015 2015 [Stephen, Hawking] 1 James Bowen A Street Cat Named Bob 23 Jan 2013 2013 [James, Bowen] 2 Peter Wohlleben The Hidden Life of Trees 22 Nov 2019 2019 [Peter, Wohlleben] 3 Raynor Winn The Salt Path 31 Jan 2019 2019 [Raynor, Winn] 4 Catherine D. Hughes Little Kids First Big Book of Dinosaurs 16 Aug 2018 2018 [Catherine, D., Hughes] NLTK PredictionI used the name dataset from heredef gender_features(word): return {&#39;last_letter&#39;:word[-1]} # preparing a list of examples and corresponding class labels.labeled_names = ([(name, &#39;Female&#39;) for name in names.words(&#39;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Names/Dataset2/Female.txt&#39;)]+ [(name, &#39;Male&#39;) for name in names.words(&#39;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Names/Dataset2/Male.txt&#39;)]) random.shuffle(labeled_names)# we use the feature extractor to process the names data.featuresets = [(gender_features(n), gender) for (n, gender)in labeled_names] # Divide the resulting list of feature# sets into a training set and a test set.train_set, test_set = featuresets[500:], featuresets[:500] # The training set is used to # train a new &quot;naive Bayes&quot; classifier.classifier = nltk.NaiveBayesClassifier.train(train_set) # 76% accuracy#print(nltk.classify.accuracy(classifier, train_set)) # Accuract is 0.7461467177257799for i in range(len(authors)): #iterate over rows # Get the name name = authors[&#39;FirstName&#39;][i][0] # If the authors&#39; name is in our name dataset check gender if name in open(&#39;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Names/Dataset2/AllNames.txt&#39;).read(): gender = classifier.classify(gender_features(name)) authors[&quot;Gender&quot;][i] = str(gender) # If the name is empty print unknown elif name == 0: authors[&quot;Gender&quot;][i] = &quot;Unknown&quot; # If the name is not in the database print unknown else: authors[&quot;Gender&quot;][i] = &quot;Unknown&quot; print(&#39;Done&#39;)# Check the table. Now it should have a designated/predicted gender for each authorauthors.head(5) authors titles date year FirstName Gender 0 Stephen Hawking A Brief History Of Time 20 Jan 2015 2015 [Stephen, Hawking] Male 1 James Bowen A Street Cat Named Bob 23 Jan 2013 2013 [James, Bowen] Male 2 Peter Wohlleben The Hidden Life of Trees 22 Nov 2019 2019 [Peter, Wohlleben] Male 3 Raynor Winn The Salt Path 31 Jan 2019 2019 [Raynor, Winn] Male 4 Catherine D. Hughes Little Kids First Big Book of Dinosaurs 16 Aug 2018 2018 [Catherine, D., Hughes] Female Save the Resultauthors.to_csv(&#39;/Users/nat/Desktop/Code/Code Projects/Book-Gender/Data/Bookdepository/NaturalHistory-All-Gender.csv&#39;)Create Stat &amp;amp; Plot All The Data# You can observe the data with these commands#authors.describe()import plotly.express as px# Book number per year (the below will give a pandas series)years_no= authors.groupby(&#39;year&#39;)[&#39;Gender&#39;].count() # Convert pandas series into a dataframedf_years = pd.DataFrame(years_no)df_years.reset_index(inplace=True)# Drop the books that had publishing dates in the future. df_years = df_years[df_years[&quot;year&quot;].str.contains(&quot;2024&quot;)==False]df_years = df_years[df_years[&quot;year&quot;].str.contains(&quot;2023&quot;)==False]df_years = df_years[df_years[&quot;year&quot;].str.contains(&quot;2022&quot;)==False] # I will also remove 2022 as we are in the beginnign of this year# Plot the datafig = px.line(df_years, x=df_years[&#39;year&#39;], y=df_years[&#39;Gender&#39;], title=&#39;Number of Published Book&#39;)fig.show()Gender Gap in the all (retrieved) nature book publishedf = authors[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] # All: 3465m = authors[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] # All: 5836u = authors[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;] # All: 659stat = pd.DataFrame({ &#39;Female&#39;: [f], &#39;Male&#39;: [m] #&#39;Unknown&#39;: [u]})stat Female Male 0 3468 5833 from matplotlib import pyplot as pltmylabels = [&quot;Female&quot;, &quot;Male&quot;]mycolors = [&quot;#34595a&quot;, &quot;#a0c293&quot;]#controls default text sizeplt.rc(&#39;font&#39;, size=15)# plot size#plt.rcParams[&quot;figure.figsize&quot;] = [10, 15]#set title font to size 50plt.rc(&#39;axes&#39;, titlesize=50) plt.pie(stat, labels = mylabels, autopct =&#39;%1.1f%%&#39;, colors = mycolors, wedgeprops = {&quot;edgecolor&quot; : &quot;black&quot;, &#39;linewidth&#39;: 2, &#39;antialiased&#39;: True})#plt.legend(loc=&#39;upper left&#39;)#plt.title(&#39;Gender Gap&#39;)# Save figure#plt.savefig(&#39;/Users/nat/Desktop/gender-gap-All.png&#39;, dpi = 100)# Display the graph onto the screenplt.show() Gender Gap Yearly Analysis# Filter datadf_2020 = authors[(authors[&#39;year&#39;] == &#39;2020&#39;)]df_2015 = authors[(authors[&#39;year&#39;] == &#39;2015&#39;)]df_2010 = authors[(authors[&#39;year&#39;] == &#39;2010&#39;)]df_2005 = authors[(authors[&#39;year&#39;] == &#39;2005&#39;)]df_2000 = authors[(authors[&#39;year&#39;] == &#39;2000&#39;)]df_1995 = authors[(authors[&#39;year&#39;] == &#39;1995&#39;)]df_1990 = authors[(authors[&#39;year&#39;] == &#39;1990&#39;)]f2 = df_2020[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m2 = df_2020[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u2 = df_2020[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;] f7 = df_2015[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m7 = df_2015[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u7 = df_2015[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;]f8 = df_2010[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m8 = df_2010[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u8 = df_2010[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;]f9 = df_2005[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m9 = df_2005[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u9 = df_2005[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;]f10 = df_2000[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m10 = df_2000[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u10 = df_2000[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;]f11 = df_1995[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] m11 = df_1995[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] u11 = df_1995[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;]# Create your dataframesevery_5 = pd.DataFrame({ &#39;Year&#39;: [2020, 2015, 2010, 2005, 2000, 1995], &#39;Female&#39;: [f2, f7, f8, f9, f10, f11], &#39;Male&#39;: [m2, m7, m8, m9, m10, m11] #&#39;Unknown&#39;: [u]})every_5 Year Female Male 0 2020 358 465 1 2015 263 383 2 2010 107 225 3 2005 58 99 4 2000 31 59 5 1995 12 19 import plotly.graph_objects as go# labels={&#39;trace 0&#39;: &quot;hello&quot;, &#39;trace 1&#39;: &quot;hi&quot;}# set up plotly figurefig = go.Figure()# add line / trace 1 to figurefig.add_trace(go.Scatter( x=every_5[&#39;Year&#39;], y=every_5[&#39;Female&#39;], hovertext=every_5[&#39;Female&#39;], hoverinfo=&quot;text&quot;, marker=dict( color=&quot;black&quot; ), showlegend=True, line_width=3))# add line / trace 2 to figurefig.add_trace(go.Scatter( x=every_5[&#39;Year&#39;], y=every_5[&#39;Male&#39;], hovertext=every_5[&#39;Male&#39;], hoverinfo=&quot;text&quot;, marker=dict( color=&quot;green&quot; ), showlegend=True, line_width=3))# Source: https://www.geeksforgeeks.org/plotly-how-to-show-legend-in-single-trace-scatterplot-with-plotly-express/fig[&#39;data&#39;][0][&#39;showlegend&#39;] = Truefig[&#39;data&#39;][0][&#39;name&#39;] = &#39;Female&#39;fig[&#39;data&#39;][1][&#39;name&#39;] = &#39;Male&#39;fig.show(renderer=&quot;png&quot;)import plotly.express as pximport pandas as pdimport plotly.graph_objs as gofig = px.bar(every_5, x=every_5[&#39;Year&#39;], y=[every_5[&#39;Male&#39;], every_5[&#39;Female&#39;]], height=400, width=700, color_discrete_map= {&quot;Male&quot;: &quot;RebeccaPurple&quot;, &quot;Female&quot;: &quot;MediumPurple&quot;}, template=&quot;simple_white&quot; )fig.show(renderer=&quot;png&quot;)" }, { "title": "Gender Gap in Nature History Books (Method: Not Preferred)", "url": "/posts/Gender-Gap-In-Nature-History-Books/", "categories": "Natural Science, Gender", "tags": "gender, books, nature-history, python", "date": "2022-02-28 00:00:00 +0100", "snippet": "About This ProjectI try to read books on nature, nature history and popular science as much as I can. Then I wondered is there a gender gap in the amount of published books in these fields. I became curious about the percentage of women, trans and non-binary nature history writers.I found some good articles about gender gap in publishing. In her visual article in The Pudding, Rosie Cima (2017) analyses the the gender balance of The New York Times best seller list. This research on gender gap in academia (Holman, Stuart-Fox and Hauser, 2018) and its interactive webpage is also worth looking. Of course, in all these research gender is binary.Main Questions Where to access nature history book authors? Where to access author gender data? Is there a gender gap in the published nature history books?How to find authors‚Äô gender identity?There are some services that libraries can use like Novelist (which is a division of EBSCO now) that provides detailed information about books and their authors. In 2020 Novelist broadened their database to include trans and non-binary to their database (Reno, 2020). This service is available in Novelist Plus but my university is not subscribed to that. So is there another database for author gender identities? Enters VIAF.What is VIAF?You can read more about VIAF here. It is a database that combines information from multiple authorities including libraries.Why I use it?There are personal pages for many authors in VIAF. Each page includes information about the authors‚Äô publication and personal data, including their gender. Check Urusla K. Le Guin‚Äôs page here for example.Downsides? Not all authors have gender data in the database Binary gender systemThe gender category for authors is binary. Check for instance one of my favorite non-binary authors page, Annalee Newitz. Annalee is categorised as ‚ÄúFemale‚Äù. So here is a shout out to VIAF: Please update your database to include more than two genders!(Not that anyone will here me from this tiny blog buuut‚Ä¶.ü§®. Yeap, my rant is finished now. Moving on.)Where to download?I downloaded their date (here). The unzipped ‚Äútxt.gz‚Äù file is 9,37 GB.How to decrease the file size to make quicker analysis?When I observed the VIAF dataset, I realised that a single author had dosens of rows and I only needed a single row containing the authors‚Äô VIAF webpage link (which is accompanied by a wikipedia link). So lets only keep the rows that has a wikipedia link.Obviously a problematic approach as there might be many authors in my dataset without a wiki page. But nevertheless this helped me to decreased the file size from ~ 9 GB to ~ 235 MB.What if VIAF doesn‚Äôt have the author‚Äôs gender?Then I can use Natural Language Toolkit to predict the authors‚Äô gender from their first name. NTLK helps us with statistical natural language processing. The prediction is, yes you guessed it, of course binary.Personal Note: I would like to understand how the text training and prediction is happening in Natural Language Toolkit. Read about NLTK for example from here.Finding the Nature History WritersThere are many book retailer webpages with decent categorisation of millions of books. Socreating a book dataset with author names and book titles is relatively easy. I used Book Depository and scraped its pages with python.They have a category called ‚ÄúNatural History‚Äù. Perfect. I fetched all the books under this category. The result was ~ 10,000 books with titles, authors and publishing year. I filtered the result to get the books published in 2021.There are no gender identity information for authors in Book Depository nor in similar platforms. And this is why I need VIAF dataset.Code Workflow Import Book Depository data Import VIAF data Cross check and match if an author listed in the Book Depository data exists in VIAF dataset. If yes: Go to authors page and get their gender info If no: Try to predict the authors‚Äô gender from their first name by using Natural Language Toolkit Summarise the findingsResult There are 982 books published in 2021 within ‚ÄúNature History‚Äù category. I retrieved the author gender of~ 32% of these books from VIAF page. For the remaining ~68% of the books/authors I used Natural Language Processing to predict their gender from their first names. This method couldn‚Äôt define gender of 15% of the authors . The results show (excluding the undetermined authors) that 6 out of 10 natural history books that were published during 2021 are written by male authors.However it‚Äôs difficult to fully rely on these results. There are numerous things to improve in this project besides finding a database with broader gender identities. These are: Re-consider the use of VIAF database. Just to find gender data for ~30% of the authors the project processing time increases dramatically. Improving the Natural Language Toolkit (NTLK) results to increase the binary-gender prediction accuracy.What I Learned? There are very few database that has authors‚Äô gender identity. The freely accessible database VIAF has binary gender system. Using VIAF and retrieving authors‚Äô gender significantly slowed down the project processing time. Moreover, I only managed to retrieve 30% of the nature history authors‚Äô gender data from VIAF. For the rest 70% of the authors I had to predict their gender from their first name by using Natural Language Toolkit (NLTK) python library. Despite possible lower accuracy it might be worth to only use Natural Language Toolkit and predict authors‚Äô (binary) gender from their first names. In fact, I analysed all the books published in nature history between 1995 and 2020. This method was much more efficient. Read about it here. The code is here.Codeimport requestsimport pandas as pdimport datetimefrom bs4 import BeautifulSoupBookdepository DatasetThis dataset is a result of webscraping. I fetched books that were published under natural history category in bookdepository webpage. The result was ~ 10,000 books with titles, authors and publishing year. I filtered this data and retrieved the natural history books for each year between 2015 and 2021. In this project I will check only the books published 2021 = 982 books.# Import Bookdepository CSVbooks = pd.read_csv(&quot;.../Bookdepository/NaturalHistory-Bookdepository-2021.csv&quot;, dtype=str)books.head(5) Unnamed: 0 authors titles date year 0 17 Merlin Sheldrake Entangled Life 02 Sep 2021 2021 1 35 Jeremy Clarkson Diddly Squat 11 Nov 2021 2021 2 38 Lia Leendertz The Almanac 02 Sep 2021 2021 3 43 Sosuke Natsukawa The Cat Who Saved Books 16 Sep 2021 2021 4 62 James Stewart Dinosaur Therapy 24 Aug 2021 2021 Virtual International Authority File (VIAF) DatasetThis is the dataset downloaded from here. The unzipped file is 9,37 GB.When I observed the VIAF dataset, I realised that a single author had dosens of rows and I only needed a single row containing the authors‚Äô VIAF webpage link. So lets simplify the csv file by only keeping the rows that has a VIAF link. Decreased the file size from ~ 9 GB to ~ 235 MB.# Import VIAF CSV (I changed the way I edited VIAF file. This is the 2nd version)viaf_db = pd.read_csv(&quot;.../Viaf/Viaf-simple.csv&quot;, dtype=str)viaf_db.head(5) Unnamed: 0 viaf info Name 0 11 http://viaf.org/viaf/10001407 Wikipedia@https://cs.wikipedia.org/wiki/Pavel_... Pavel Hrach 1 70 http://viaf.org/viaf/100109330 Wikipedia@https://fr.wikipedia.org/wiki/Emile_... Emile de Meester de Ravestein 2 121 http://viaf.org/viaf/100144403 Wikipedia@https://cy.wikipedia.org/wiki/Teresa... Teresa Magalh√£es 3 246 http://viaf.org/viaf/100177876 Wikipedia@https://nl.wikipedia.org/wiki/Guilla... Guillaume Caoursin 4 331 http://viaf.org/viaf/100208187 Wikipedia@https://ru.wikipedia.org/wiki/–†–∏—à–ª–µ,... –†–∏—à–ª–µ, –°–µ–∑–∞—Ä-–ü—å–µ—Ä Check VIAF links by using the Bookdepository author namesAll we need to do is to cross check and link the book depository authors to their VIAF pages (if it exists).Note: The blow process takes a long time. Grab a coffee : )links = []author_with_viaf = []books_with_viaf = []names_to_predict = []author_without_viaf = []books_without_viaf = []for i in range(len(books[&#39;authors&#39;])): # Find whether the name in bookdeposit dataset exists in VIAF dataset name = books[&#39;authors&#39;][i] #print(i) boolean_finding = viaf_db[&#39;Name&#39;].str.contains(name, case=False).any() # True or False # If its true get the viaf link if boolean_finding == True: df2 = viaf_db[(viaf_db[&#39;Name&#39;].str.contains(name, case=False))] viaf_link = df2[&#39;viaf&#39;].loc[df2.index[0]] links.append(viaf_link) author_with_viaf.append(books[&#39;authors&#39;][i]) books_with_viaf.append(books[&#39;titles&#39;][i]) else: names_to_predict.append(name) author_without_viaf.append(books[&#39;authors&#39;][i]) books_without_viaf.append(books[&#39;titles&#39;][i])print(&quot;Cross checking finished. If a VIAF link exists for an author it was saved in the list.&quot;) # Convert lists to dataframe and then save locally (Saving the files makes it easier to test)# Create dataframe from the listszipped1 = list(zip(author_with_viaf, books_with_viaf, links))zipped2 = list(zip(author_without_viaf, books_without_viaf, names_to_predict))viaf_authors = pd.DataFrame(zipped1, columns=[&#39;Author&#39;, &#39;Book&#39;, &#39;Links&#39;])predict_authors = pd.DataFrame(zipped2, columns=[&#39;Author&#39;, &#39;Book&#39;, &#39;Names&#39;])# storing these dataframes in a csv fileviaf_authors.to_csv(r&#39;.../Viaf/Viaf-author-links-2021.csv&#39;) #, index = Nonepredict_authors.to_csv(r&#39;.../Viaf/Author_names_to_predict-2021.csv&#39;) b = len(books)v = len(viaf_authors)p = len(predict_authors)print(&quot;There are {} books published in 2021 within Nature History category. This code managed to link {} of these book authors to their personal VIAF pages from which we will retrieve author gender information. We need to predict the gender for the remaining {} authors.&quot;.format(b,v,p))There are 982 books published in 2021 within Nature History category. This code managed to link 311 of these book authors to their personal VIAF pages from which we will retrieve author gender information. We need to predict the gender for the remaining 671 authors.Fetching gender data from VIAF pages with Beautifulsoup# Import CSVviaf_authors = pd.read_csv(&quot;.../Viaf/Viaf-author-links-2021.csv&quot;, dtype=str)# Create an empty column for genderviaf_authors[&quot;Gender&quot;] = &quot;&quot;# Drop unnecessary columnsviaf_authors.drop(&#39;Unnamed: 0&#39;, axis=1, inplace=True)viaf_authors.head(5) Author Book Links Gender 0 Merlin Sheldrake Entangled Life http://viaf.org/viaf/31157340763509922488 1 Jeremy Clarkson Diddly Squat http://viaf.org/viaf/102195935 2 James Stewart Dinosaur Therapy http://viaf.org/viaf/121702213 3 James Rebanks English Pastoral http://viaf.org/viaf/311766886 4 Raynor Winn The Wild Silence http://viaf.org/viaf/74154921352863592450 for i in range(len(viaf_authors)): name = viaf_authors[&quot;Author&quot;][i] #print(i) url = viaf_authors[&#39;Links&#39;][i] response = requests.get(url) html = response.content soup = BeautifulSoup(html, &quot;lxml&quot;) gender = [] for item in soup.select(&quot;div.subsection&quot;): text = item.text.strip() text = text.replace(&#39;\\n&#39;,&#39;&#39;) text = text.replace(&#39;\\t&#39;,&#39; &#39;) text = text.replace(&#39; &#39;,&#39;&#39;) gender.append(text) text = text[0].split(&#39;Nationality&#39;) text = str(text[0]) f = &quot;Female&quot; in gender[0] m = &quot;Male&quot; in gender[0] if f == True: viaf_authors[&quot;Gender&quot;][i] = &quot;Female&quot; #print(name, &quot;Female&quot;) elif m == True: viaf_authors[&quot;Gender&quot;][i] = &quot;Male&quot; #print(name, &quot;Male&quot;) else: viaf_authors[&quot;Gender&quot;][i] = &quot;Unknown&quot; #print(name, &quot;Unknown&quot;)# Export to CSV viaf_authors.to_csv(&#39;.../Viaf/Viaf-authors-gender-2021.csv&#39;)# Import CSVauthors = pd.read_csv(&quot;.../Viaf/Viaf-authors-gender-2021.csv&quot;, dtype=str)# Drop unnecessary columnsauthors.drop(&#39;Unnamed: 0&#39;, axis=1, inplace=True)authors.head(5) Author Book Links Gender 0 Merlin Sheldrake Entangled Life http://viaf.org/viaf/31157340763509922488 NaN 1 Jeremy Clarkson Diddly Squat http://viaf.org/viaf/102195935 NaN 2 James Stewart Dinosaur Therapy http://viaf.org/viaf/121702213 NaN 3 James Rebanks English Pastoral http://viaf.org/viaf/311766886 NaN 4 Raynor Winn The Wild Silence http://viaf.org/viaf/74154921352863592450 NaN Statistics (VIAF)f1 = authors[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] # 113 &amp;lt;class &#39;numpy.int64&#39;&amp;gt;m1 = authors[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] # 202u1 = authors[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;] # 24data1 = pd.DataFrame({ &#39;female&#39;: [f1], &#39;male&#39;: [m1], &#39;unknown&#39;: [u1]})data1 female male unknown 0 106 175 30 Predicting gender from first name by using Natural Language ProcessingFor those Book Depository author names that didn‚Äôt match with any VIAF data we can use NLTK to train and predict the binary gender of the authors from their first names.Of course the method is problematic. Not just because it predicts and assigns the wrong gender to authors‚Äô first name but also because of the binary gender assumption. In this method there is no room for non-binary and trans people. I tested the trained NLTK with my name and with 80% accuracy the code said I was a male. Well, I‚Äôm not.# Source: https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/import randomfrom nltk.corpus import namesimport nltk# Import CSVpredict_authors = pd.read_csv(&quot;.../Viaf/Author_names_to_predict-2021.csv&quot;, dtype=str)# Create a colum for first namespredict_authors[&quot;FirstName&quot;] = &quot;&quot;predict_authors[&#39;FirstName&#39;] = predict_authors[&#39;Names&#39;].str.split(&#39; &#39;, expand=True)# Create an empty column for genderpredict_authors[&quot;Gender&quot;] = &quot;&quot;# Drop unnecessary columnspredict_authors.drop(&#39;Unnamed: 0&#39;, axis=1, inplace=True)predict_authors.head(5) Author Book Names FirstName Gender 0 Lia Leendertz The Almanac Lia Leendertz Lia 1 Sosuke Natsukawa The Cat Who Saved Books Sosuke Natsukawa Sosuke 2 Lelia Wanick Salgado Sebastiao Salgado. GENESIS Lelia Wanick Salgado Lelia 3 Rodney Habib The Forever Dog Rodney Habib Rodney 4 Andrew Cotter Dog Days Andrew Cotter Andrew import randomfrom nltk.corpus import namesimport nltk# Source1: ntlk prediction: https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/def gender_features(word): return {&#39;last_letter&#39;:word[-1]} # preparing a list of examples and corresponding class labels.#labeled_names = ([(name, &#39;Female&#39;) for name in names.words(&#39;/Users/nat/Desktop/FemaleMix.rtf&#39;)]+ #[(name, &#39;Male&#39;) for name in names.words(&#39;/Users/nat/Desktop/MaleMix.rtf&#39;)]) # 133 flabeled_names = ([(name, &#39;Female&#39;) for name in names.words(&#39;.../Names/female.txt&#39;)]+ [(name, &#39;Male&#39;) for name in names.words(&#39;.../Names/male.txt&#39;)])random.shuffle(labeled_names)# we use the feature extractor to process the names data.featuresets = [(gender_features(n), gender) for (n, gender)in labeled_names] # Divide the resulting list of feature# sets into a training set and a test set.train_set, test_set = featuresets[500:], featuresets[:500] # The training set is used to # train a new &quot;naive Bayes&quot; classifier.classifier = nltk.NaiveBayesClassifier.train(train_set)predicted_gender = []# Use the new dataframe with Nan instead of &quot;book_database&quot; dataframe because emoty cells cause issues.for i in range(len(predict_authors)): #iterate over rows # Get the name name = predict_authors[&#39;FirstName&#39;][i] #print(name) # If the name is in the database check gender if name in open(&#39;.../Names/Allnames&#39;).read(): gender = classifier.classify(gender_features(name)) predict_authors[&quot;Gender&quot;][i] = str(gender) #print(gender) # If the name is empty print unknown elif name == 0: predict_authors[&quot;Gender&quot;][i] = &quot;Unknown&quot; #print(&quot;Empty&quot;) # If the name is not in the database print unknown else: #print (&#39;Unknown&#39;) predict_authors[&quot;Gender&quot;][i] = &quot;Unknown&quot; #pd.set_option(&#39;display.max_rows&#39;, None)predict_authors.head(5) Author Book Names FirstName Gender 0 Lia Leendertz The Almanac Lia Leendertz Lia Female 1 Sosuke Natsukawa The Cat Who Saved Books Sosuke Natsukawa Sosuke Unknown 2 Lelia Wanick Salgado Sebastiao Salgado. GENESIS Lelia Wanick Salgado Lelia Female 3 Rodney Habib The Forever Dog Rodney Habib Rodney Female 4 Andrew Cotter Dog Days Andrew Cotter Andrew Male Statistics (NTLK)f2 = predict_authors[&#39;Gender&#39;].value_counts()[&#39;Female&#39;] # 133 OR 186m2 = predict_authors[&#39;Gender&#39;].value_counts()[&#39;Male&#39;] # 296u2 = predict_authors[&#39;Gender&#39;].value_counts()[&#39;Unknown&#39;] # 85data2 = pd.DataFrame({ &#39;female&#39;: [f2], &#39;male&#39;: [m2], &#39;unknown&#39;: [u2]})data2 female male unknown 0 247 311 113 Summing it all up# Total of VIAF gender &amp;amp; predicted gendertotal_f = f1 + f2total_m = m1 + m2total_u = u1 + u2total_with_unknown = pd.DataFrame({ &#39;Female&#39;: [total_f], &#39;Male&#39;: [total_m], &#39;Unknown&#39;: [total_u]})total_without_unknown = pd.DataFrame({ &#39;Female&#39;: [total_f], &#39;Male&#39;: [total_m]})total_without_unknown Female Male 0 353 486 PlotsLets plot the results with and without the unknown gender.from matplotlib import pyplot as pltmylabels = [&quot;Female&quot;, &quot;Male&quot;]mycolors = [&quot;#34595a&quot;, &quot;#a0c293&quot;]#controls default text sizeplt.rc(&#39;font&#39;, size=15)# plot sizeplt.rcParams[&quot;figure.figsize&quot;] = [10, 15]#set title font to size 50plt.rc(&#39;axes&#39;, titlesize=50) plt.pie(total_without_unknown, labels = mylabels, autopct =&#39;%1.1f%%&#39;, colors = mycolors, wedgeprops = {&quot;edgecolor&quot; : &quot;black&quot;, &#39;linewidth&#39;: 2, &#39;antialiased&#39;: True})#plt.legend(loc=&#39;upper left&#39;)plt.title(&#39;Gender Gap&#39;)# Save figureplt.savefig(&#39;/Users/nat/Desktop/gender-gap.png&#39;, dpi = 100)# Display the graph onto the screenplt.show() fig = plt.figure()mylabels1 = [&quot;Female&quot;, &quot;Male&quot;, &quot;Unknown&quot;]mycolors1 = [&quot;#34595a&quot;, &quot;#a0c293&quot;, &quot;#f0cfc3&quot;]mylabels2 = [&quot;Female&quot;, &quot;Male&quot;]mycolors2 = [&quot;#34595a&quot;, &quot;#a0c293&quot;]ax1 = fig.add_axes([0, 0, .5, .5], aspect=1)ax1.pie(total_with_unknown, labels=mylabels1, radius = 1.2, autopct =&#39;%1.1f%%&#39;, colors = mycolors1, wedgeprops = {&quot;edgecolor&quot; : &quot;black&quot;, &#39;linewidth&#39;: 2, &#39;antialiased&#39;: True})ax2 = fig.add_axes([.5, .0, .5, .5], aspect=1)ax2.pie(total_without_unknown, labels=mylabels2, radius = 1.2, autopct =&#39;%1.1f%%&#39;, colors = mycolors2, wedgeprops = {&quot;edgecolor&quot; : &quot;black&quot;, &#39;linewidth&#39;: 2, &#39;antialiased&#39;: True})ax1.set_title(&#39;Gender Gap&#39;)#ax2.set_title(&#39;Title for ax2&#39;)plt.show()References Cima, R. (2017) The Gender Balance of The New York Times Best Seller List, The Pudding. Available at: https://pudding.cool/2017/06/best-sellers/index.html (Accessed: 1 March 2022). Holman, L., Stuart-Fox, D. and Hauser, C.E. (2018) ‚ÄòThe gender gap in science: How long until women are equally represented?‚Äô, PLOS Biology. Edited by C. Sugimoto, 16(4), p. e2004956. doi:10.1371/journal.pbio.2004956. Reno, A. (2020) Author gender identity added to NoveList, EBSCO Information Services, Inc. Available at: https://www.ebsco.com/blogs/novelist/author-gender-identity-added-novelist (Accessed: 25 February 2022).Linked Sources The code for analysing gender of nature history writers by using VIAF dataset code Web scraping Book Depository Webpage code How to work with 10GB VIAF dataset code Get Gender Information From a Persons‚Äô VIAF page code" }, { "title": "Assessing seagrass vulnerability to coastal erosion in Madagascar", "url": "/posts/Thesis/", "categories": "Climate Change, Coastal Erosion", "tags": "python, satellite, gis, remote sensing, seagrasss, madagascar, google earth engine, jupyter notebook, earth observation", "date": "2021-04-01 00:00:00 +0200", "snippet": "My interdisciplinary thesis was part of both¬†Department of Physical Geography¬†&amp;amp;¬†Department of Ecology, Environment and Plant Sciences (DEEP)¬†in Stockholm University.The entire project was automated using Python in Jupyter Notebook (Github¬†link). See for the demonstrative video and the abstract below.Below are some of the libraries I used: Google Earth Engine Python API for accessing Landsat Collections Pandas and GeoPandas to read data and shapefiles NumPy for array operations GDAL for raster to vector conversion Ipyleaflet for interactive mapping Matplotlib for plotting Geoplot for creating a choropleth mapAssessment of coastal erosion to create a seagrass vulnerability index in northwestern Madagascar using automated quantification analysisAbstractThe seagrass extent has been declining globally. The human activities that are most likely to cause seagrass loss are those which affect the water quality and clarity. However, turbidity following coastal erosion is often left out from marine ecosystem vulnerability indices. This study quantified the coastal erosion for Tsimipaika Bay in northwestern Madagascar by using change detection analysis of satellite imageries. The annual coastal erosion data was then used to create an index for seagrass vulnerability to turbidity following coastal erosion. Considering that the height of seagrass species plays an important role in their survival following turbidity, the seagrass vulnerability index (SVI) was based on two factors; seagrass species height and their distance to the nearest possible erosion place. The results for the coastal erosion showed that the amount of erosion was particularly high in 1996, 2001 and 2009 for Tsimipaika Bay. The highest erosion occurred in 2001 with a land loss area of about 6.2 km2 . The SVI maps revealed that 40% of the seagrass communities had minimum mean SVI values in 2001 and 50% had the maximum mean SVI during the year 2009. This study showed that it is possible to use coastal erosion to measure seagrass vulnerability; however, the index requires configuration such as including the total amount of annual coastal erosion and incorporating bathymetric data. The entire project was built and automated in Jupyter Notebook using Python programming language, which creates a ground for future studies to develop and modify the project.KeywordsSeagrass; Coastal erosion; Vulnerability index; Python; Google Earth Engine; Deforestation; Turbidity; Mangrove; Madagascar" }, { "title": "Atlas of Inspiring Protests", "url": "/posts/Atlas/", "categories": "Cartography, Illustration", "tags": "photoshop, illustration, atlas, map, protest", "date": "2021-03-23 00:00:00 +0100", "snippet": "In 2017, I started creating high-quality map posters for some well-known individual or national protests. It was a great joy and each project had its own cartographic challenges.Tools used in these projects: Open Street Map for vector data Adobe Photoshop for illustrationDanuta Danielsson, SwedenTank Man, ChinaBlack Monday, PolandGezi Protest, Turkey" }, { "title": "Change Detection: Comparing Methods for Natural Hazard Quantification using Remote Sensing and GIS", "url": "/posts/Change-Detection/", "categories": "Natural and Human-Made Hazards, Tsunami", "tags": "satellite, gis, remote sensing", "date": "2021-02-01 00:00:00 +0100", "snippet": "Software used in this project: ENVI for atmospheric correction SNAP for Sentinel 2 analysis ArcMap for NDVI analysisI recently finished my master‚Äôs programme in¬†Geomatics with Remote Sensing and GIS.¬†I don‚Äôt think I would be exaggerating if I say that it was the best times of my life.This was on of the many projects I did for the course ‚ÄúApplied remote sensing and GIS for landscape analysis‚Äù on 2018. I wanted to see how different methods impacted the natural hazard quantification. Now I can see how I could have improved this project. It would have been better to focus on a single method to go deeper in to the theory To analyse a natural disaster event which had ground truth data for the damaged areas. This way I could have computed accuracy of each method.I‚Äôm not going to share all the details. Here are some of my notes from this project.Change Detection MethodsThe aim of my project is to implement and compare different change detection techniques to see how they perform compared to each other in highlighting and quantifying the damaged area. We can divide change detection in remote sensing into five groups (Mouat, Mahin, &amp;amp; Lancaster, 1993). During the this time I had an opportunity to try and compare four groups and in total six different change detection techniques that are; Visual interpretation True color comparison Image Algebra (difference and ratio images) Single Band NDVI Euclidean Distance Transformation and data reduction PCA Classification Supervised Classification Statistical (This method was kept outside the study scope due to time limit)As the essence of change detection is to compare pixels from before and after images the following preprocessing steps are very important (Campbell &amp;amp; Wynne, 2011). The images should be; - Acquired by the same sensor - Acquired during the same season - Well co registered - Free of clouds - Atmospherically correctedStudy AreaMy study area is Palu city, on the Indonesian island of Sulawesi, where a tsunami struck right after the earthquake on September 28, 2018. I used two radiometrically corrected Sentinel 2 Level 1C images (‚ÄúUser Guides Sentinel-2‚Äù n.d.). I used ENVI to do the atmospheric correction.Project Workflow The workflow has eight steps. In the end by intersecting the results from the change detection techniques with the road and building shapefiles (step 8), I‚Äôm trying to quantify the post tsunami-earthquake hazard.Troubleshooting // Discoveries Sen2cor plugin in SNAP was buggy. The automatically &amp;amp; manually calculated NDVI values in ArcMap were different. I realised a pixel shift between September and October images. Results References Campbell, J. B., &amp;amp; Wynne, R. H. (2011). Introduction to Remote Sensing, Fifth Edition. Guilford Press. Mouat, D. A., Mahin, G. G., &amp;amp; Lancaster, J. (1993). Remote sensing techniques in the analysis of change detection. Geocarto International, 8(2), 39‚Äì50.¬†https://doi.org/10.1080/10106049309354407 User Guides Sentinel-2. (n.d.). Retrieved November 4, 2018, from¬†https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/processing-levels/level-1¬†" }, { "title": "Estimating the Sea Level Rise for 2100 in Osaka, Japan", "url": "/posts/Sea-Level-Osaka/", "categories": "Climate Change, Sea Level Rise", "tags": "gis, remote sensing, climate change, sea level, japan, ipcc, dem, arcgis", "date": "2021-02-01 00:00:00 +0100", "snippet": "This was a project for GIS Visualisation course in 2019. I wrote 21 page report but this text will be a ‚Äúsuper summary‚Äù of it with no excessive details.Software: ArcGIS ExcelData: Japan shapefiles including population data from Japanese Government Statistics Agency DEM 30m from USGS Maximum tide data from Japan Oceanographic Data Center Yearly average sea level anomaly data from Japan Meteorological Agency IPCC reportClimate change, urbanisation, and land subsidence are already affecting the lives of millions in the coastal cities by raising the sea level and increasing the risk of coastal flooding. Governments will need to take further expensive actions to secure lives to prevent coastal flooding.Future sea levels are calculated by complex models that accounts for regional and global data such as land subsidence, land uplift, atmospheric and oceanic circulations. extreme water level etc.In this project I used a simplified equation to predict future sea level for 2100.WL = SLR + MT + SLA + HS WL = Water level SLR = Global mean sea level rise for 2100 MT = Increase in the maximum tide SLA = Increase in mean sea level anomaly HS = Human-induced subsidenceStudy AreaThe Japan archipelago faces the Pacific Ocean to the east and the Sea of Japan to the west. It is, by nature, very vulnerable to sea level rise, land subsidence and consequently to tides and typhoons. The city of Osaka, in central Japan, is open to the Osaka Bay on the west side and it lies on a flat alluvial plain. The elevation can be as low as 1 to 3 meters in many places.Calculating Future Sea Level in OsakaThe equation needs four parameters to be determined. The first one, global mean sea level rise (SLR), is directly acquired from the IPCC reports RPC 8.5 scenario without any further calculation.The increase in the maximum tide (MT) is calculated using the hourly tide records dataset from JODC (Japan Oceanographic Data Center, n.d.). The records date back to 1961 and are recorded hourly until 2015. The tide level for 2100 is calculated by creating a linear trend line equation. The increase in maximum tide is then calculated by subtracting the 2100 data from 2015.The data for yearly average sea level anomaly is taken from JMA (Japan Meteorological Agency, 2018) and dates back to 1960 and continues until 2017. The anomaly level for 2100 is calculated using a linear equation and the change (SLA) is computed by subtracting 2100 level from 2017.The subsidence (HS) in Osaka is observed to be zero (stopped), as a result of many prevention laws, as stated in the World Bank report (EX Corporation, 1996).Overall, the water level in Osaka in 2100 is the sum of these parameters:WL = 0.63 + 0.7 + 0.1 + 0 = 1.43 meterConsidering the most extreme IPCC scenario, the global mean sea level predictions for 2100 is 0.63 meters (IPCC, 2015). So my simplistic formula seems way too off. But nevertheless I will compute the submerged areas assuming this water level.ResultsThe calculations show that by 2100 an area about 24 km2 could be submerged in Osaka as a result of rising sea levels. 92% of the submerged areas would be urban areas where as 3% would be vegetated areas and 2% rice fields. Additional calculations reveal that 10% of the buildings could be exposed to flooding after the increase in sea level. This means that more than 210.000 people are estimated to be effected in this sea level rise scenario.DiscussionThe result for mean the sea level rise may change according to the data sources. For instance, although the report from World Bank (1996) indicated no land subsidence in Osaka, the Japanese Ministry of the Environment reported a subsidence about 1.4 cm/year (Ministry of Environment, 2005). The report is Japanese so I couldn‚Äôt entirely be sure.The formula used in this project is simplistic and the accuracy seems low (considering IPCC report). The effects of defense systems like dikes and storm surge barriers should be considered for a future study. Moreover, to increase the accuracy, a coastal vulnerability index can be used to calculate the coastal vulnerability to sea level rise (Ramieri et al., 2011).References EX Corporation. (1996). Japan‚Äôs experience in urban environmental management : Main Report (English) (Departmental Working Paper ‚Ññ17942). Washington, DC: World Bank. IPCC (2015)¬†Climate change 2014: Synthesis Report. Geneva, Switzerland: Intergovernmental Panel on Climate Change. Japan Meteorological Agency. (2018). Sea level. Retrieved January 14, 2019, from¬†https://www.data.jma.go.jp/gmd/kaiyou/english/sl_trend/¬†sea_level_around_japan.html Japan Oceanographic Data Center. (n.d.). Tide Data Search. Retrieved January 14, 2019, from¬†http://jdoss1.jodc.go.jp/vpage/tide.html Ministry of Environment. (2005). Overview of Ground Subsidence in Japan in FY 2005. Ministry of Environment. Retrieved from¬†https://www.env.go.jp/¬†press/7835.html Ramieri, E., Hartley, A., Barbanti, A., Duarte Santos, F., Gomes, A., Hilden, M.(2011). Methods for assessing coastal vulnerability to climate change (ETC CCA Technical Paper). European Environment Agency. Retrieved from¬†https://climate-adapt.eea.europa.eu/metadata/tools/¬†coastal-vulnerability-index-2013-cvi" } ]
